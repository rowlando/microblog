These are the typical constraints of software in traditional data centres:

1. Getting an environment. You're joking!
2. Code deployment. Come back next week once you've spoken to a a release manager.
3. Tests. Where has all the test data gone? I can't test, need a refresh from prod.
4. Approval. Sorry, need to ask team x,y & z for approval, as they'll probably need to make a change too.

What can you do?

1. Provision environments using code, a.k.a. infrastructure as code.
2. Deploy your code using code, a.k.a. build & deployment pipelines. 
3. Test your code using code. Store app code, test code and test data together. And version it.
4. Decouple your software into smaller chunks, aiming for independent deployments.

These practices come under DevOps or Continuous Delivery movement. Given that compute power is a commodity offered by private or public cloud, these practices, commonly referred to as DevOps practices, are simple to achieve for all types of organisation.

When is this really hard?

One example is changing the direction of enterprise legacy applications, with characteristics such as big ball of mud architectures, [databases masquerading as applications](/1591522217/) and software systems so complicated that a two-pizza sized multi-discliplinary team cannot hope to build, deploy and operate without the support of specialised teams. This loosely describes an ERP system where I work. The investment required to change the [application inertia](https://bernardgolden.com/cloud-computing-and-application-inertia/) is significant. Before investing, it's critical to have a vision of what comes after a lift and shift to cloud otherwise the significant investment doesn't yield any value, except for a notional step towards greater cloud adoption.
